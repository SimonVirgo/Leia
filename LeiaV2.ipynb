{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read and set API keys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Open and read the config file\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config_data = json.load(config_file)\n",
    "\n",
    "# Retrieve the API key from the config data\n",
    "api_key = config_data['api_key']\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "import logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO) #DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from llama_index.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "\n",
    "# you can set a tokenizer directly, or optionally let it default\n",
    "# to the same tokenizer that was used previously for token counting\n",
    "# NOTE: The tokenizer should be a function that takes in text and returns a list of tokens\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(\"gpt-4\").encode,\n",
    "    verbose=True  # set to true to see usage printed to the console\n",
    "    )\n",
    "callback_manager = CallbackManager([token_counter])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.storage.kvstore.simple_kvstore:Loading llama_index.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "Loading llama_index.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "DEBUG:fsspec.local:open file: /Users/simonvirgo/PycharmProjects/Leia/storage/docstore.json\n",
      "open file: /Users/simonvirgo/PycharmProjects/Leia/storage/docstore.json\n",
      "DEBUG:llama_index.storage.kvstore.simple_kvstore:Loading llama_index.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
      "Loading llama_index.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
      "DEBUG:fsspec.local:open file: /Users/simonvirgo/PycharmProjects/Leia/storage/index_store.json\n",
      "open file: /Users/simonvirgo/PycharmProjects/Leia/storage/index_store.json\n",
      "DEBUG:llama_index.vector_stores.simple:Loading llama_index.vector_stores.simple from ./storage/vector_store.json.\n",
      "Loading llama_index.vector_stores.simple from ./storage/vector_store.json.\n",
      "DEBUG:fsspec.local:open file: /Users/simonvirgo/PycharmProjects/Leia/storage/vector_store.json\n",
      "open file: /Users/simonvirgo/PycharmProjects/Leia/storage/vector_store.json\n",
      "DEBUG:llama_index.graph_stores.simple:Loading llama_index.graph_stores.simple from ./storage/graph_store.json.\n",
      "Loading llama_index.graph_stores.simple from ./storage/graph_store.json.\n",
      "DEBUG:fsspec.local:open file: /Users/simonvirgo/PycharmProjects/Leia/storage/graph_store.json\n",
      "open file: /Users/simonvirgo/PycharmProjects/Leia/storage/graph_store.json\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.chat_engine.condense_question:\n",
      "\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAuthenticationError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m chat_engine \u001B[38;5;241m=\u001B[39m index\u001B[38;5;241m.\u001B[39mas_chat_engine()\n\u001B[0;32m----> 2\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mchat_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwhat is LiquidEarth?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/llama_index/chat_engine/condense_question.py:127\u001B[0m, in \u001B[0;36mCondenseQuestionChatEngine.chat\u001B[0;34m(self, message, chat_history)\u001B[0m\n\u001B[1;32m    124\u001B[0m chat_history \u001B[38;5;241m=\u001B[39m chat_history \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_chat_history\n\u001B[1;32m    126\u001B[0m \u001B[38;5;66;03m# Generate standalone question from conversation context and last message\u001B[39;00m\n\u001B[0;32m--> 127\u001B[0m condensed_question \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_condense_question\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchat_history\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    129\u001B[0m log_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQuerying with: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcondensed_question\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    130\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(log_str)\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/llama_index/chat_engine/condense_question.py:97\u001B[0m, in \u001B[0;36mCondenseQuestionChatEngine._condense_question\u001B[0;34m(self, chat_history, last_message)\u001B[0m\n\u001B[1;32m     94\u001B[0m chat_history_str \u001B[38;5;241m=\u001B[39m messages_to_history_str(chat_history)\n\u001B[1;32m     95\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(chat_history_str)\n\u001B[0;32m---> 97\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_service_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm_predictor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     98\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_condense_question_prompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquestion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlast_message\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    100\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchat_history\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchat_history_str\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/llama_index/llm_predictor/base.py:123\u001B[0m, in \u001B[0;36mLLMPredictor.predict\u001B[0;34m(self, prompt, **prompt_args)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    122\u001B[0m     formatted_prompt \u001B[38;5;241m=\u001B[39m prompt\u001B[38;5;241m.\u001B[39mformat(llm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_llm, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprompt_args)\n\u001B[0;32m--> 123\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_llm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcomplete\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatted_prompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    124\u001B[0m     output \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mtext\n\u001B[1;32m    126\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(output)\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/llama_index/llms/openai.py:72\u001B[0m, in \u001B[0;36mOpenAI.complete\u001B[0;34m(self, prompt, **kwargs)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     71\u001B[0m     complete_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_complete\n\u001B[0;32m---> 72\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcomplete_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/llama_index/llms/openai.py:183\u001B[0m, in \u001B[0;36mOpenAI._complete\u001B[0;34m(self, prompt, **kwargs)\u001B[0m\n\u001B[1;32m    180\u001B[0m     max_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_max_token_for_prompt(prompt)\n\u001B[1;32m    181\u001B[0m     all_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m max_tokens\n\u001B[0;32m--> 183\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mcompletion_with_retry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_chat_model\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_is_chat_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mall_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m text \u001B[38;5;241m=\u001B[39m response[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchoices\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m CompletionResponse(\n\u001B[1;32m    192\u001B[0m     text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[1;32m    193\u001B[0m     raw\u001B[38;5;241m=\u001B[39mresponse,\n\u001B[1;32m    194\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/llama_index/llms/openai_utils.py:123\u001B[0m, in \u001B[0;36mcompletion_with_retry\u001B[0;34m(is_chat_model, max_retries, **kwargs)\u001B[0m\n\u001B[1;32m    120\u001B[0m     client \u001B[38;5;241m=\u001B[39m get_completion_endpoint(is_chat_model)\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m client\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 123\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_completion_with_retry\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/tenacity/__init__.py:289\u001B[0m, in \u001B[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001B[0;34m(*args, **kw)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(f)\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped_f\u001B[39m(\u001B[38;5;241m*\u001B[39margs: t\u001B[38;5;241m.\u001B[39mAny, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: t\u001B[38;5;241m.\u001B[39mAny) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m t\u001B[38;5;241m.\u001B[39mAny:\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/tenacity/__init__.py:379\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    377\u001B[0m retry_state \u001B[38;5;241m=\u001B[39m RetryCallState(retry_object\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, fn\u001B[38;5;241m=\u001B[39mfn, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 379\u001B[0m     do \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretry_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    381\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/tenacity/__init__.py:314\u001B[0m, in \u001B[0;36mBaseRetrying.iter\u001B[0;34m(self, retry_state)\u001B[0m\n\u001B[1;32m    312\u001B[0m is_explicit_retry \u001B[38;5;241m=\u001B[39m fut\u001B[38;5;241m.\u001B[39mfailed \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fut\u001B[38;5;241m.\u001B[39mexception(), TryAgain)\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_explicit_retry \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mretry(retry_state)):\n\u001B[0;32m--> 314\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfut\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mafter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mafter(retry_state)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:451\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    449\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[1;32m    450\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m--> 451\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_condition\u001B[38;5;241m.\u001B[39mwait(timeout)\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:403\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 403\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    404\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    405\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[1;32m    406\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/tenacity/__init__.py:382\u001B[0m, in \u001B[0;36mRetrying.__call__\u001B[0;34m(self, fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(do, DoAttempt):\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 382\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n\u001B[1;32m    384\u001B[0m         retry_state\u001B[38;5;241m.\u001B[39mset_exception(sys\u001B[38;5;241m.\u001B[39mexc_info())  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/llama_index/llms/openai_utils.py:121\u001B[0m, in \u001B[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001B[0;34m(**kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    120\u001B[0m     client \u001B[38;5;241m=\u001B[39m get_completion_endpoint(is_chat_model)\n\u001B[0;32m--> 121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001B[0m, in \u001B[0;36mCompletion.create\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 25\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m TryAgain \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     27\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m>\u001B[39m start \u001B[38;5;241m+\u001B[39m timeout:\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:149\u001B[0m, in \u001B[0;36mEngineAPIResource.create\u001B[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[1;32m    137\u001B[0m ):\n\u001B[1;32m    138\u001B[0m     (\n\u001B[1;32m    139\u001B[0m         deployment_id,\n\u001B[1;32m    140\u001B[0m         engine,\n\u001B[1;32m    141\u001B[0m         timeout,\n\u001B[1;32m    142\u001B[0m         stream,\n\u001B[1;32m    143\u001B[0m         headers,\n\u001B[1;32m    144\u001B[0m         request_timeout,\n\u001B[1;32m    145\u001B[0m         typed_api_type,\n\u001B[1;32m    146\u001B[0m         requestor,\n\u001B[1;32m    147\u001B[0m         url,\n\u001B[1;32m    148\u001B[0m         params,\n\u001B[0;32m--> 149\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__prepare_create_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[43m        \u001B[49m\u001B[43mapi_key\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapi_base\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapi_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapi_version\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morganization\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    153\u001B[0m     response, _, api_key \u001B[38;5;241m=\u001B[39m requestor\u001B[38;5;241m.\u001B[39mrequest(\n\u001B[1;32m    154\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    155\u001B[0m         url,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    160\u001B[0m         request_timeout\u001B[38;5;241m=\u001B[39mrequest_timeout,\n\u001B[1;32m    161\u001B[0m     )\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m    164\u001B[0m         \u001B[38;5;66;03m# must be an iterator\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:106\u001B[0m, in \u001B[0;36mEngineAPIResource.__prepare_create_request\u001B[0;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m timeout \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    104\u001B[0m     params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m MAX_TIMEOUT\n\u001B[0;32m--> 106\u001B[0m requestor \u001B[38;5;241m=\u001B[39m \u001B[43mapi_requestor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAPIRequestor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[43m    \u001B[49m\u001B[43mapi_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[43m    \u001B[49m\u001B[43mapi_base\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mapi_base\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[43m    \u001B[49m\u001B[43mapi_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mapi_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    110\u001B[0m \u001B[43m    \u001B[49m\u001B[43mapi_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mapi_version\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    111\u001B[0m \u001B[43m    \u001B[49m\u001B[43morganization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morganization\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    113\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mclass_url(engine, api_type, api_version)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m    115\u001B[0m     deployment_id,\n\u001B[1;32m    116\u001B[0m     engine,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    124\u001B[0m     params,\n\u001B[1;32m    125\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/openai/api_requestor.py:130\u001B[0m, in \u001B[0;36mAPIRequestor.__init__\u001B[0;34m(self, key, api_base, api_type, api_version, organization)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    123\u001B[0m     key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    127\u001B[0m     organization\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    128\u001B[0m ):\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_base \u001B[38;5;241m=\u001B[39m api_base \u001B[38;5;129;01mor\u001B[39;00m openai\u001B[38;5;241m.\u001B[39mapi_base\n\u001B[0;32m--> 130\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_key \u001B[38;5;241m=\u001B[39m key \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdefault_api_key\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_type \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    132\u001B[0m         ApiType\u001B[38;5;241m.\u001B[39mfrom_str(api_type)\n\u001B[1;32m    133\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m api_type\n\u001B[1;32m    134\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m ApiType\u001B[38;5;241m.\u001B[39mfrom_str(openai\u001B[38;5;241m.\u001B[39mapi_type)\n\u001B[1;32m    135\u001B[0m     )\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi_version \u001B[38;5;241m=\u001B[39m api_version \u001B[38;5;129;01mor\u001B[39;00m openai\u001B[38;5;241m.\u001B[39mapi_version\n",
      "File \u001B[0;32m~/PycharmProjects/Leia/venv3.10/lib/python3.10/site-packages/openai/util.py:186\u001B[0m, in \u001B[0;36mdefault_api_key\u001B[0;34m()\u001B[0m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m openai\u001B[38;5;241m.\u001B[39mapi_key\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 186\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m openai\u001B[38;5;241m.\u001B[39merror\u001B[38;5;241m.\u001B[39mAuthenticationError(\n\u001B[1;32m    187\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo API key provided. You can set your API key in code using \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mopenai.api_key = <API-KEY>\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mopenai.api_key_path = <PATH>\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    188\u001B[0m     )\n",
      "\u001B[0;31mAuthenticationError\u001B[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://onboard.openai.com for details, or email support@openai.com if you have any questions."
     ]
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine()\n",
    "response = chat_engine.chat(\"what is LiquidEarth?\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GPT 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4,callback_manager=callback_manager)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### create Prompt Template"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from llama_index import Prompt\n",
    "# define custom Prompt\n",
    "TEMPLATE_STR = (\n",
    "    \"You are Leia, the LiquidEarth Intelligent Assistant. You are helping a user with a question about LiquidEarth. You are very smart and friendly and always in a great mood.\\n\"\n",
    "    \"In LiquidEarth, a 'Space' and a 'Project are the same thing. We have provided Documentation on the software and further information below. In some cases the metadata includes a 'Control' Field that points to a UI element in the app associated to the described functionality. this is only for internal use. when describing controls to the user, use descriptions and names from the text, not the 'control' values. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Answer the question for a human to understand. Additionally, return the 'Control' properties in the order of operations in the following form at the end of your response: [[control1], [control2], ...]. Append the list to your response without further comment. If no controls are found, do not comment it. Never include any controls that are not specified in the Metadata Field in the provided documentation. Do not interpret any controls from the text body. If the answer requires multiple steps, describe each step in detail. Given this information, please answer the question: {query_str}\\n\"\n",
    ")\n",
    "QA_TEMPLATE = Prompt(TEMPLATE_STR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(service_context=service_context_gpt4, text_qa_template=QA_TEMPLATE, retriever_mode=\"embedding\",callback_manager=callback_manager)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#dirty hack: trying to increase the context size\n",
    "query_engine.retriever._similarity_top_k = 6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/embeddings\n",
      "DEBUG:openai:api_version=None data='{\"input\": [\"hello,how can i create a project and add some data?\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "api_version=None data='{\"input\": [\"hello,how can i create a project and add some data?\"], \"model\": \"text-embedding-ada-002\", \"encoding_format\": \"base64\"}' message='Post details'\n",
      "DEBUG:urllib3.util.retry:Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)\n",
      "Converted retries value: 2 -> Retry(total=2, connect=None, read=None, redirect=None, status=None)\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.openai.com:443\n",
      "Starting new HTTPS connection (1): api.openai.com:443\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/embeddings HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=23 request_id=35f0ef04183e8c5a93603f66331c53bb response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=23 request_id=35f0ef04183e8c5a93603f66331c53bb response_code=200\n",
      "DEBUG:llama_index.indices.utils:> Top 6 nodes:\n",
      "> [Node 43487e28-21f9-49a6-99d7-16ba7891d9f1] [Similarity score:             0.845407] Create a New Project\n",
      "\n",
      "Assigned: Fabian Stamm\n",
      "Category: Getting Started\n",
      "Control: NULL\n",
      "Priority: 5\n",
      "...\n",
      "> [Node 9b0ad209-fb0b-48e7-974d-79e0908350f8] [Similarity score:             0.820621] Long Description\n",
      "\n",
      "The server explorer lists all the projects, Spaces, and data that you have acce...\n",
      "> [Node a049808b-dba3-4f66-b57d-ad6e43e0bb8b] [Similarity score:             0.812652] Import Data\n",
      "\n",
      "Assigned: Fabian Stamm\n",
      "Category: Getting Started\n",
      "Control: ExplorerDataImporter\n",
      "Prior...\n",
      "> [Node f1b5f7d6-dbc8-4574-b80c-693ac3fa015d] [Similarity score:             0.806995] Edit Data\n",
      "\n",
      "Assigned: Fabian Stamm\n",
      "Category: Explorer Controls\n",
      "Control: EditProjectData\n",
      "Priority: ...\n",
      "> [Node 9b1c9faa-1d57-45d3-903e-6e784245c814] [Similarity score:             0.801789] Content Selection & Navigation\n",
      "\n",
      "After loading a project space containing data into your local wor...\n",
      "> [Node 10c2168e-70df-4bbe-98fe-0e2e2744a4f1] [Similarity score:             0.80099] Long Description\n",
      "\n",
      "Create a new element with respect to the explorer section you have opened:\n",
      "\n",
      "- I...\n",
      "> Top 6 nodes:\n",
      "> [Node 43487e28-21f9-49a6-99d7-16ba7891d9f1] [Similarity score:             0.845407] Create a New Project\n",
      "\n",
      "Assigned: Fabian Stamm\n",
      "Category: Getting Started\n",
      "Control: NULL\n",
      "Priority: 5\n",
      "...\n",
      "> [Node 9b0ad209-fb0b-48e7-974d-79e0908350f8] [Similarity score:             0.820621] Long Description\n",
      "\n",
      "The server explorer lists all the projects, Spaces, and data that you have acce...\n",
      "> [Node a049808b-dba3-4f66-b57d-ad6e43e0bb8b] [Similarity score:             0.812652] Import Data\n",
      "\n",
      "Assigned: Fabian Stamm\n",
      "Category: Getting Started\n",
      "Control: ExplorerDataImporter\n",
      "Prior...\n",
      "> [Node f1b5f7d6-dbc8-4574-b80c-693ac3fa015d] [Similarity score:             0.806995] Edit Data\n",
      "\n",
      "Assigned: Fabian Stamm\n",
      "Category: Explorer Controls\n",
      "Control: EditProjectData\n",
      "Priority: ...\n",
      "> [Node 9b1c9faa-1d57-45d3-903e-6e784245c814] [Similarity score:             0.801789] Content Selection & Navigation\n",
      "\n",
      "After loading a project space containing data into your local wor...\n",
      "> [Node 10c2168e-70df-4bbe-98fe-0e2e2744a4f1] [Similarity score:             0.80099] Long Description\n",
      "\n",
      "Create a new element with respect to the explorer section you have opened:\n",
      "\n",
      "- I...\n",
      "DEBUG:openai:message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "message='Request to OpenAI API' method=post path=https://api.openai.com/v1/chat/completions\n",
      "DEBUG:openai:api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"You are Leia, the LiquidEarth Intelligent Assistant. You are helping a user with a question about LiquidEarth. You are very smart and friendly and always in a great mood.\\\\nIn LiquidEarth, a \\'Space\\' and a \\'Project are the same thing. We have provided Documentation on the software and further information below. In some cases the metadata includes a \\'Control\\' Field that points to a UI element in the app associated to the described functionality. this is only for internal use. when describing controls to the user, use descriptions and names from the text, not the \\'control\\' values. \\\\n---------------------\\\\nControl: NULL\\\\n\\\\nCreate a New Project\\\\n\\\\nAssigned: Fabian Stamm\\\\nCategory: Getting Started\\\\nControl: NULL\\\\nPriority: 5\\\\nProduct Version: All\\\\nStatus: Preliminary\\\\nlast updated on: November 27, 2022\\\\n\\\\nFor creating a new project space, follow these steps:\\\\n\\\\n1. Open up the server explorer on the left.\\\\n2. Click the ****Plus**** button to create a new project. It will appear at the bottom of your server explorer list.\\\\n3. Optional: To adapt basic information and project metadata (such as project name and owner), use *********Edit Data********* in the inspector.\\\\n    1. Once you have set all required fields correctly, *******Save******* them to accept the information.\\\\n        \\\\n        !new_project2.png\\\\n        \\\\n4. Via inspector, use *************Load Project************* to load it into your local workspace (3D Workspace). It will appear in your 3D workspace and listed in the Local Explorer. \\\\n    - Note that a respective message will appear in the activity log and that the loading bar will indicate the loading progress.\\\\n    - If you are working online, the project will also be automatically uploaded to the cloud or the remote server you are connected to.\\\\n\\\\nThe now-created empty project space in your 3D Workspace will by default be visualized by a bounding box that covers the extent defined when you created the project. If you didn\\\\u2019t adjust the extent, the standard is set to a cubic extent with a one-meter length for X, Y, and Z.\\\\n\\\\nThe project space provides a framework for you to start importing data. So let us continue with that.\\\\n\\\\n**********************Next Step: Import Data**\\\\n\\\\nLong Description\\\\n\\\\nThe server explorer lists all the projects, Spaces, and data that you have access to remotely, i.e. which are saved either online in the cloud, or on a remote server. You have access to projects and data that you either created or uploaded yourself, or to which your User Account has been granted access by an authorized User.\\\\n\\\\nThe server explorer lists each project with the following basic information:\\\\n\\\\n- Thumbnail  (if applicable and created by a user)\\\\n- Project name\\\\n- Project owner\\\\n- Date of creation\\\\n\\\\nTo create a new project, click the Explorer New button below the explorer ist.\\\\n\\\\nTo delete a project, select it in the list, then click the Delete a Space button below the explorer ist.\\\\n\\\\nControl: ExplorerDataImporter\\\\n\\\\nImport Data\\\\n\\\\nAssigned: Fabian Stamm\\\\nCategory: Getting Started\\\\nControl: ExplorerDataImporter\\\\nPriority: 5\\\\nProduct Version: All\\\\nStatus: Preliminary\\\\nTooltip: Open the \\\\u201cadd data\\\\u201d wizard to import datasets into your space \\\\nlast updated on: June 9, 2023\\\\n\\\\nYou can import data into any Space (including project spaces) loaded in your 3D Workspace.\\\\n\\\\nTo start importing data, select the target space and click on the *Import Data* button in the bottom left of the Local Explorer . \\\\n\\\\n!Screenshot 2022-11-25 at 09.55.44.png\\\\n\\\\nThis will open up the ******************Data Import Wizard****************** which will guide you through the import process for different types of data. For more details on  which data types are currently supported by LiquidEarth, check out the entry Import Formats.\\\\n\\\\n!Screenshot 2022-11-25 at 10.08.51.png\\\\n\\\\nThe import process depends on the type of data you want to add:\\\\n\\\\n \\\\n\\\\n- **Meshes**\\\\n    - Wavefront OBJ\\\\n    - DXF\\\\n    - OMF\\\\n    - glTF\\\\n- **Boreholes** (coming soon)\\\\n- **Volumes** (coming soon)\\\\n    \\\\n    \\\\n\\\\nChoose the type of data you want to import and follow the steps elaborated in the wizard. After finalizing the process, the data will be imported into the selected project space. This might happen quickly or take some time depending on the size of the data. A respective message will appear in the activity log and the loading bar will indicate the importing progress.\\\\n\\\\nWith the data loaded, you can now visualize and interact with it using the tools available to you in LiquidEarth (see: Visualization & Basic Interactions and Advanced Interactions).\\\\n\\\\n**Next Step:** Delete Data\\\\n\\\\nControl: EditProjectData\\\\n\\\\nEdit Data\\\\n\\\\nAssigned: Fabian Stamm\\\\nCategory: Explorer Controls\\\\nControl: EditProjectData\\\\nPriority: 10\\\\nProduct Version: All\\\\nStatus: Not started\\\\n\\\\nContent Selection & Navigation\\\\n\\\\nAfter loading a project space containing data into your local workspace (3D Workspace), it will be visualized in 3D. By default, the project space will be displayed with a bounding box which can be used to select the space by clicking on it. The data inside of a space, such as meshes, can also be selected by directly clicking on it. In general, there are various ways to navigate between different selectable content, such as by using the Breadcrumb Trail or simply via the Local Explorer. To learn more about this, see the entry on **Content Navigation**.\\\\n\\\\n!Untitled\\\\n\\\\nIf you have any content selected, it will be highlighted in the workspace and the Inspector (or its respective quickbar) will display options and tools contextual to the content.\\\\n\\\\nLong Description\\\\n\\\\nCreate a new element with respect to the explorer section you have opened:\\\\n\\\\n- In the Server Explorer you can create a new project or Space.\\\\n- In the Local Explorer you can create one of several elements in your local Workspace.\\\\n- In the Annotations Explorer you can create a new Annotation.\\\\n- In the Analysis Explorer you will create a new Probe.\\\\n---------------------\\\\nAnswer the question for a human to understand. Additionally, return the \\'Control\\' properties in the order of operations in the following form at the end of your response: [[control1], [control2], ...]. Append the list to your response without further comment. If no controls are found, do not comment it. Never include any controls that are not specified in the Metadata Field in the provided documentation. Do not interpret any controls from the text body. If the answer requires multiple steps, describe each step in detail. Given this information, please answer the question: hello,how can i create a project and add some data?\\\\n\"}], \"stream\": false, \"model\": \"gpt-4\", \"temperature\": 0.0, \"max_tokens\": null}' message='Post details'\n",
      "api_version=None data='{\"messages\": [{\"role\": \"user\", \"content\": \"You are Leia, the LiquidEarth Intelligent Assistant. You are helping a user with a question about LiquidEarth. You are very smart and friendly and always in a great mood.\\\\nIn LiquidEarth, a \\'Space\\' and a \\'Project are the same thing. We have provided Documentation on the software and further information below. In some cases the metadata includes a \\'Control\\' Field that points to a UI element in the app associated to the described functionality. this is only for internal use. when describing controls to the user, use descriptions and names from the text, not the \\'control\\' values. \\\\n---------------------\\\\nControl: NULL\\\\n\\\\nCreate a New Project\\\\n\\\\nAssigned: Fabian Stamm\\\\nCategory: Getting Started\\\\nControl: NULL\\\\nPriority: 5\\\\nProduct Version: All\\\\nStatus: Preliminary\\\\nlast updated on: November 27, 2022\\\\n\\\\nFor creating a new project space, follow these steps:\\\\n\\\\n1. Open up the server explorer on the left.\\\\n2. Click the ****Plus**** button to create a new project. It will appear at the bottom of your server explorer list.\\\\n3. Optional: To adapt basic information and project metadata (such as project name and owner), use *********Edit Data********* in the inspector.\\\\n    1. Once you have set all required fields correctly, *******Save******* them to accept the information.\\\\n        \\\\n        !new_project2.png\\\\n        \\\\n4. Via inspector, use *************Load Project************* to load it into your local workspace (3D Workspace). It will appear in your 3D workspace and listed in the Local Explorer. \\\\n    - Note that a respective message will appear in the activity log and that the loading bar will indicate the loading progress.\\\\n    - If you are working online, the project will also be automatically uploaded to the cloud or the remote server you are connected to.\\\\n\\\\nThe now-created empty project space in your 3D Workspace will by default be visualized by a bounding box that covers the extent defined when you created the project. If you didn\\\\u2019t adjust the extent, the standard is set to a cubic extent with a one-meter length for X, Y, and Z.\\\\n\\\\nThe project space provides a framework for you to start importing data. So let us continue with that.\\\\n\\\\n**********************Next Step: Import Data**\\\\n\\\\nLong Description\\\\n\\\\nThe server explorer lists all the projects, Spaces, and data that you have access to remotely, i.e. which are saved either online in the cloud, or on a remote server. You have access to projects and data that you either created or uploaded yourself, or to which your User Account has been granted access by an authorized User.\\\\n\\\\nThe server explorer lists each project with the following basic information:\\\\n\\\\n- Thumbnail  (if applicable and created by a user)\\\\n- Project name\\\\n- Project owner\\\\n- Date of creation\\\\n\\\\nTo create a new project, click the Explorer New button below the explorer ist.\\\\n\\\\nTo delete a project, select it in the list, then click the Delete a Space button below the explorer ist.\\\\n\\\\nControl: ExplorerDataImporter\\\\n\\\\nImport Data\\\\n\\\\nAssigned: Fabian Stamm\\\\nCategory: Getting Started\\\\nControl: ExplorerDataImporter\\\\nPriority: 5\\\\nProduct Version: All\\\\nStatus: Preliminary\\\\nTooltip: Open the \\\\u201cadd data\\\\u201d wizard to import datasets into your space \\\\nlast updated on: June 9, 2023\\\\n\\\\nYou can import data into any Space (including project spaces) loaded in your 3D Workspace.\\\\n\\\\nTo start importing data, select the target space and click on the *Import Data* button in the bottom left of the Local Explorer . \\\\n\\\\n!Screenshot 2022-11-25 at 09.55.44.png\\\\n\\\\nThis will open up the ******************Data Import Wizard****************** which will guide you through the import process for different types of data. For more details on  which data types are currently supported by LiquidEarth, check out the entry Import Formats.\\\\n\\\\n!Screenshot 2022-11-25 at 10.08.51.png\\\\n\\\\nThe import process depends on the type of data you want to add:\\\\n\\\\n \\\\n\\\\n- **Meshes**\\\\n    - Wavefront OBJ\\\\n    - DXF\\\\n    - OMF\\\\n    - glTF\\\\n- **Boreholes** (coming soon)\\\\n- **Volumes** (coming soon)\\\\n    \\\\n    \\\\n\\\\nChoose the type of data you want to import and follow the steps elaborated in the wizard. After finalizing the process, the data will be imported into the selected project space. This might happen quickly or take some time depending on the size of the data. A respective message will appear in the activity log and the loading bar will indicate the importing progress.\\\\n\\\\nWith the data loaded, you can now visualize and interact with it using the tools available to you in LiquidEarth (see: Visualization & Basic Interactions and Advanced Interactions).\\\\n\\\\n**Next Step:** Delete Data\\\\n\\\\nControl: EditProjectData\\\\n\\\\nEdit Data\\\\n\\\\nAssigned: Fabian Stamm\\\\nCategory: Explorer Controls\\\\nControl: EditProjectData\\\\nPriority: 10\\\\nProduct Version: All\\\\nStatus: Not started\\\\n\\\\nContent Selection & Navigation\\\\n\\\\nAfter loading a project space containing data into your local workspace (3D Workspace), it will be visualized in 3D. By default, the project space will be displayed with a bounding box which can be used to select the space by clicking on it. The data inside of a space, such as meshes, can also be selected by directly clicking on it. In general, there are various ways to navigate between different selectable content, such as by using the Breadcrumb Trail or simply via the Local Explorer. To learn more about this, see the entry on **Content Navigation**.\\\\n\\\\n!Untitled\\\\n\\\\nIf you have any content selected, it will be highlighted in the workspace and the Inspector (or its respective quickbar) will display options and tools contextual to the content.\\\\n\\\\nLong Description\\\\n\\\\nCreate a new element with respect to the explorer section you have opened:\\\\n\\\\n- In the Server Explorer you can create a new project or Space.\\\\n- In the Local Explorer you can create one of several elements in your local Workspace.\\\\n- In the Annotations Explorer you can create a new Annotation.\\\\n- In the Analysis Explorer you will create a new Probe.\\\\n---------------------\\\\nAnswer the question for a human to understand. Additionally, return the \\'Control\\' properties in the order of operations in the following form at the end of your response: [[control1], [control2], ...]. Append the list to your response without further comment. If no controls are found, do not comment it. Never include any controls that are not specified in the Metadata Field in the provided documentation. Do not interpret any controls from the text body. If the answer requires multiple steps, describe each step in detail. Given this information, please answer the question: hello,how can i create a project and add some data?\\\\n\"}], \"stream\": false, \"model\": \"gpt-4\", \"temperature\": 0.0, \"max_tokens\": null}' message='Post details'\n",
      "DEBUG:urllib3.connectionpool:https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "https://api.openai.com:443 \"POST /v1/chat/completions HTTP/1.1\" 200 None\n",
      "DEBUG:openai:message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=25503 request_id=342c6428fa22178a873b30612c83e7c2 response_code=200\n",
      "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=25503 request_id=342c6428fa22178a873b30612c83e7c2 response_code=200\n",
      "DEBUG:llama_index.llm_predictor.base:Hello! Here's how you can create a project and add data in LiquidEarth:\n",
      "\n",
      "**Creating a New Project:**\n",
      "\n",
      "1. Open the server explorer on the left side of your screen.\n",
      "2. Click the Plus button at the bottom of your server explorer list to create a new project.\n",
      "3. (Optional) You can adapt basic information and project metadata such as the project name and owner by using the Edit Data option in the inspector. Once you've set all required fields correctly, click Save to accept the information.\n",
      "4. Use the Load Project option in the inspector to load your project into your local workspace (3D Workspace). Your project will appear in your 3D workspace and will be listed in the Local Explorer. If you're working online, the project will also be automatically uploaded to the cloud or the remote server you're connected to.\n",
      "\n",
      "**Adding Data to Your Project:**\n",
      "\n",
      "1. Select the target space (your newly created project) and click on the Import Data button in the bottom left of the Local Explorer.\n",
      "2. This will open the Data Import Wizard which will guide you through the import process for different types of data. Choose the type of data you want to import and follow the steps provided by the wizard.\n",
      "3. After finalizing the process, the data will be imported into your selected project space. This might happen quickly or take some time depending on the size of the data.\n",
      "\n",
      "Once you've imported your data, you can visualize and interact with it using the tools available in LiquidEarth.\n",
      "\n",
      "[[NULL], [ExplorerDataImporter], [EditProjectData]]\n",
      "Hello! Here's how you can create a project and add data in LiquidEarth:\n",
      "\n",
      "**Creating a New Project:**\n",
      "\n",
      "1. Open the server explorer on the left side of your screen.\n",
      "2. Click the Plus button at the bottom of your server explorer list to create a new project.\n",
      "3. (Optional) You can adapt basic information and project metadata such as the project name and owner by using the Edit Data option in the inspector. Once you've set all required fields correctly, click Save to accept the information.\n",
      "4. Use the Load Project option in the inspector to load your project into your local workspace (3D Workspace). Your project will appear in your 3D workspace and will be listed in the Local Explorer. If you're working online, the project will also be automatically uploaded to the cloud or the remote server you're connected to.\n",
      "\n",
      "**Adding Data to Your Project:**\n",
      "\n",
      "1. Select the target space (your newly created project) and click on the Import Data button in the bottom left of the Local Explorer.\n",
      "2. This will open the Data Import Wizard which will guide you through the import process for different types of data. Choose the type of data you want to import and follow the steps provided by the wizard.\n",
      "3. After finalizing the process, the data will be imported into your selected project space. This might happen quickly or take some time depending on the size of the data.\n",
      "\n",
      "Once you've imported your data, you can visualize and interact with it using the tools available in LiquidEarth.\n",
      "\n",
      "[[NULL], [ExplorerDataImporter], [EditProjectData]]\n",
      "LLM Prompt Token Usage: 1393\n",
      "LLM Completion Token Usage: 318\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"hello,how can i create a project and add some data?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Here's how you can create a project and add data in LiquidEarth:\n",
      "\n",
      "**Creating a New Project:**\n",
      "\n",
      "1. Open the server explorer on the left side of your screen.\n",
      "2. Click the Plus button at the bottom of your server explorer list to create a new project.\n",
      "3. (Optional) You can adapt basic information and project metadata such as the project name and owner by using the Edit Data option in the inspector. Once you've set all required fields correctly, click Save to accept the information.\n",
      "4. Use the Load Project option in the inspector to load your project into your local workspace (3D Workspace). Your project will appear in your 3D workspace and will be listed in the Local Explorer. If you're working online, the project will also be automatically uploaded to the cloud or the remote server you're connected to.\n",
      "\n",
      "**Adding Data to Your Project:**\n",
      "\n",
      "1. Select the target space (your newly created project) and click on the Import Data button in the bottom left of the Local Explorer.\n",
      "2. This will open the Data Import Wizard which will guide you through the import process for different types of data. Choose the type of data you want to import and follow the steps provided by the wizard.\n",
      "3. After finalizing the process, the data will be imported into your selected project space. This might happen quickly or take some time depending on the size of the data.\n",
      "\n",
      "Once you've imported your data, you can visualize and interact with it using the tools available in LiquidEarth.\n",
      "\n",
      "[[NULL], [ExplorerDataImporter], [EditProjectData]]\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chat with a prompt template (ToDo)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "custom_prompt = Prompt(\"\"\"\\\n",
    "Given a conversation (between Human and Assistant) and a follow up message from Human, \\\n",
    "rewrite the message to be a standalone question that captures all relevant context \\\n",
    "from the conversation.\n",
    "\n",
    "<Chat History>\n",
    "{chat_history}\n",
    "\n",
    "<Follow Up Message>\n",
    "{question}\n",
    "\n",
    "<Standalone question>\n",
    "\"\"\")\n",
    "\n",
    "# list of (human_message, ai_message) tuples\n",
    "custom_chat_history = [\n",
    "    (\n",
    "        'Hello assistant, we are having a insightful discussion about Paul Graham today.',\n",
    "        'Okay, sounds good.'\n",
    "    )\n",
    "]\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    condense_question_prompt=custom_prompt,\n",
    "    chat_history=custom_chat_history,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## print token usage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Tokens:  0 \n",
      " LLM Prompt Tokens:  1393 \n",
      " LLM Completion Tokens:  318 \n",
      " Total LLM Token Count:  1711\n"
     ]
    }
   ],
   "source": [
    "print('Embedding Tokens: ', token_counter.total_embedding_token_count, '\\n',\n",
    "      'LLM Prompt Tokens: ', token_counter.prompt_llm_token_count, '\\n',\n",
    "      'LLM Completion Tokens: ', token_counter.completion_llm_token_count, '\\n',\n",
    "      'Total LLM Token Count: ', token_counter.total_llm_token_count)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
