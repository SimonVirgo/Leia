{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain import OpenAI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Open and read the config file\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config_data = json.load(config_file)\n",
    "\n",
    "# Retrieve the API key from the config data\n",
    "api_key = config_data['api_key']\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## create a new index from scratch\n",
    "check [LlamaHub](https://llamahub.ai/) for different helpers to create an index.\n",
    "export notion pages, etc. to markdown and put them into the data folder.\n",
    "For now exclude the images, they currently blow up the chunk size limit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load you data into 'Documents' a custom type by LlamaIndex\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('./data', recursive=True).load_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create an index of your documents\n",
    "from llama_index import GPTSimpleVectorIndex\n",
    "\n",
    "#try building index all at once\n",
    "index = GPTSimpleVectorIndex(documents, chunk_size_limit=1024)\n",
    "\n",
    "#try building index in chunks\n",
    "index = GPTSimpleVectorIndex([])\n",
    "for doc in documents:\n",
    "    print(doc.doc_id)\n",
    "    index.insert(doc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the index\n",
    "index.save('index')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## append to an existing index\n",
    "put additional data into the addData folder and run the following code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Load the index\n",
    "from llama_index import GPTSimpleVectorIndex\n",
    "\n",
    "index = GPTSimpleVectorIndex.load_from_disk('index')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Load you data into 'Documents' a custom type by LlamaIndex\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('./addData', recursive=True).load_data()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63028a56-382b-438b-aafc-1b7f13b711e2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 100 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 56 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380b3453-c97e-4777-9344-cd39dbc463c6\n",
      "407933f4-f64c-4f92-a7de-d75419009851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 53 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75c1138f-fc50-4dcd-a969-31aec85504a5\n",
      "3d22be67-32f6-4a1d-bcc8-44b5a054dc4a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 70 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 139 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07a20853-5787-490b-b9a4-d7bc010fcb0a\n",
      "6fd83fc6-2a11-47ab-93fe-0adad4596aed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 53 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43d5f58b-d70e-4b05-965d-5928dda5a59a\n",
      "aa05fb6e-9992-4c76-910a-f6bb126b7637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 190 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0ec58061-8762-4d3d-8410-3ec69b0917f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 90 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68ff95fe-0d66-4b49-b7ad-794183914353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 149 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 58 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fd622feb-ba7d-4019-bbae-f146578512a0\n",
      "4ad88e0e-1c76-40c6-b21c-3ccb9cfe7e73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 54 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 115 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8a444a93-2c09-4d5c-91ab-97914a41f2f5\n",
      "276054e2-6b67-4e92-8438-7f9d20ad739e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 130 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35f15354-afd7-4d47-a43a-7bb9b584f66c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 48 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 109 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00ad58d9-0ab9-46cb-b41a-4a49323fd949\n",
      "6072bb65-906e-4e90-9018-100ac9ab76c6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 195 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 150 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7d1cff77-0c44-4e6f-834d-02856f51a1ff\n",
      "d6fd2203-0c60-4283-ae0e-adec078b8c5a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e59074ef-3304-45fa-9219-3a62d00c3f14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 41 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 208 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9b679f6c-f739-48fc-ac59-93ae922ea232\n",
      "8704b935-2596-4979-8a97-4547a3f89cd1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 41 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 232 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbb1e992-62c4-4343-8b84-936486ecf1da\n",
      "a4a4bf93-17cd-4032-996a-dee6cfc3ba81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 101 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690c25a-597b-45cb-8849-d1a981b20acc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 509 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ee8ff4ad-5248-46d6-b629-ec85cc1236d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 413 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b4560da1-ad1d-4386-9470-8c4b1f325f07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 359 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 460 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e9c66d44-0733-43a8-98ec-9d8e4bad64cd\n",
      "ea081d96-c820-4c38-8c4a-dbf66b96c7f6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 248 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 215 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bf4ddae2-d0a9-4d63-b370-f00c1ffe096d\n",
      "4e17eec1-e591-40ea-8d3d-302b25313242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 102 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0d4b4c93-e63a-4664-9845-124ed22242a1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 120 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 112 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5879e111-0b74-4174-bed1-364f116d3859\n",
      "5d26d79d-3dbf-4acc-beb7-0df2925d4204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 134 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 163 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff788c24-fb31-4f30-80d1-df495c5ea048\n",
      "5136d646-4891-48c5-9694-84269cdbce1e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 126 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 373 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fb1c3500-bca7-47af-90d9-677b9d7f519d\n",
      "97fe1a46-94a3-4578-aa82-3e5dfccb417d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 131 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 124 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20c2007a-db05-4bb3-aab0-8f9e16bfd281\n",
      "96f17359-54c9-4860-9b78-1bec2067b09f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 127 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 99 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6a35b375-6c27-4406-95c9-e34680f14339\n",
      "dcaaa02e-54a0-40ec-bed2-2eb9793b47cd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 68 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f028c61d-f9f6-4d74-a0af-0e56ad0ac50f\n",
      "5eb9edc0-2ef5-4550-87e8-14de58b6e41e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 63 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 55 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4db80d1e-9ffc-4267-8904-54151bdf564e\n",
      "0b892f0f-7831-402a-a421-4776af9e6445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 116 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 176 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b2b59e5f-81e2-407a-993f-5ab1f73dd110\n",
      "d8c1ec49-e5cd-4c30-a7b1-56b00834b103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 107 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 103 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27f0588c-3460-4034-9502-1a37b4914cb9\n",
      "dffae8fd-6b52-4650-bfa9-372c9584ee01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202d398f-e9f3-4a10-b86f-0193db7979e9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 6976 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0667b7aa-aa30-42c8-9b2f-1ad371437b1a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 7 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 221 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b49770db-b16d-4a64-a4e1-c8576654cc08\n",
      "dbdb38b8-f90d-4d4f-9d5a-65d244e75f48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 365 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2224326b-9ba3-43d8-86b3-4b2847819011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [insert] Total embedding token usage: 83 tokens\n"
     ]
    }
   ],
   "source": [
    "# Append the documents to the index\n",
    "for doc in documents:\n",
    "    print(doc.doc_id)\n",
    "    index.insert(doc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Save the index\n",
    "index.save_to_disk('index')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
